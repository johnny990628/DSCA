task: fine_tuning_clam # use low and high resolutions (dual stream)
experiment: sim # default sim

# spliting
seed_data_split: 0 # for 5-fold evaluation
path_data_split: ./data_split/tcga_luad_merged/tcga_luad_merged-seed42-fold{}.npz

# fine tuning
csv_path: /work/u6658716/TCGA-LUAD/DSCA/data_split/tcga_luad_merged/tcga_luad_merged_path_full.csv
h5_dir: /work/u6658716/TCGA-LUAD/PATCHES/LUAD/tiles-10x-s224
slide_dir: /work/u6658716/TCGA-LUAD/DATASETS/TCGA/LUAD
ckpt_path: /work/u6658716/TCGA-LUAD/CLAM/checkpoints/conch/pytorch_model.bin
# lora_checkpoint: ./results-luad-hier/ple_conch_finetuning_5x_224/lora_checkpoint_epoch_3
lora_checkpoint:
target_patch_size: 224

# data
dataset_name: tcga_luad_merged
magnification: 10-40 # default setting, using low and high resolutions
path_patchx20: /work/u6658716/TCGA-LUAD/PATCHES/tiles-20x-s224/feats-CONCH-s224/pt_files # for cellular model: wsi
path_patchx5: /work/u6658716/TCGA-LUAD/PATCHES/tiles-20x-s224/cellular-HOVERNET-s224/pt_files # for cellular model: cell
path_coordx5: /work/u6658716/TCGA-LUAD/PATCHES/tiles-20x-s224/patches
path_label: ./data_split/tcga_luad_merged/tcga_luad_merged_path_full.csv
label_discrete: False # default setting
bins_discrete: 4 # default setting, bins to divide survival times
feat_format: pt # default setting, the format of files storing patch features
num_patch_sampling: -1 # not sampling patches

# CUDA
no_cuda: False
cuda_id: 0 # use which gpu

# seed
seed: 42

# input dim
dims: 512-256-256 # for clam
# dims: 512-256-128-64-32 # for fusion
cell_in_dim: 1024
top_k: 500

# fusion
early_fusion: coattn

# output
save_path: ./results-luad-hier/ple_conch_finetuning_10x_224 # ss
save_prediction: True

# Patch Embedding
emb_x5_backbone: conv1d # avgpool / gapool / conv1d
emb_x5_ksize: 5 # kernel size of conv1d used in low-stream
emb_x20_backbone: capool # pooling function for high-resolution patches, avgpool / gapool / conv / capool, default capool (cross-attention)
emb_x20_dw_conv: False # won't be used when capool
emb_x20_ksize: 3 # won't be used when capool

# Transformer Encoder
tra_position_emb: True # if using patch positional embedding, default True
tra_backbone: Transformer # Nystromformer / Transformer, default Transformer
tra_nhead: 2
tra_num_layers: 1 # 1 / 2, default using one layer.
tra_ksize: 3 # only used for Conv1D / Conv2D, default 3
tra_dw_conv: False # only used for Conv1D / Conv2D, default False.
tra_epsilon: 0.8 # won't be used in DSCA.

# Model Setting
join: post # using post or pre fusion for dual-stream features, default post .
fusion: fusion # the way of fusing dual-stream feature, fusion / cat, default fusion (adding dual-stream features).
pool: gap # final instance pooling, max / mean / max_mean / gap, default gap.
dropout: 0.6

# loss (default setting)
loss: survple
alpha: 0.1
reg_l1: 0.00001

# training (default setting)
batch_size: 1
num_workers: 4
epochs: 150
bp_every_iters: 338
monitor_metrics: ci
es_patience: 10
es_warmup: 0
es_start_epoch: 0
es_verbose: True

# optimizer and learning rate (default setting)
opt: adam
weight_decay: 0.0001
lr: 0.00001
opt_eps: null
opt_betas: null
opt_momentum: null
